{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Tuple\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils.scraper import get_content, extract_ipa_for_language\n",
    "from utils.file import save\n",
    "from config import OUTPUT_DIRECTORY, ANKI_CONNECT_URL, DECK_NAME, LANGUAGE, VOCAB_FIELD, N_CORES, N_JOBS_EXTRACT, N_JOBS_UPDATE, DATE_FORMAT\n",
    "\n",
    "TEST_PARSING = True\n",
    "TEST_CREATING = True\n",
    "TEST_UPDATING = True\n",
    "TEST_RETRY_UPDATING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_anki(action, **params):\n",
    "    request = {'action': action, 'params': params, 'version': 6}\n",
    "    response = requests.post(ANKI_CONNECT_URL, json=request).json()\n",
    "    if response.get('error'):\n",
    "        raise Exception(response['error'])\n",
    "    return response['result']\n",
    "\n",
    "def fetch_all_deck_names():\n",
    "    try:\n",
    "        deck_names = request_anki('deckNames')\n",
    "        print(\"Deck names:\", deck_names)\n",
    "        return deck_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching deck names: {e}\")\n",
    "        return []\n",
    "    \n",
    "# Parse dates from filenames and find most recent\n",
    "def parse_date(filename):\n",
    "    # Extract date string from \"anki@YYYYMMDD-HHMMSS.json\" or \"after_anki@YYYYMMDD-HHMMSS.json\"\n",
    "    pattern = re.compile(r\"\\w+@(\\d{8}-\\d{6}).json\")\n",
    "    match = pattern.search(filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return datetime.strptime(date_str, \"%Y%m%d-%H%M%S\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(note):\n",
    "    return note['fields'][VOCAB_FIELD]['value'].strip().strip(\"-\")\n",
    "\n",
    "def get_ipa(note):\n",
    "    return note['fields']['IPA']['value'].strip()\n",
    "\n",
    "\n",
    "def load_anki_json(filename: str):\n",
    "    if not (filename.startswith(\"anki@\") or filename.startswith(\"after_anki@\")) and not filename.endswith(\".json\"):\n",
    "        raise ValueError(f\"Invalid filename: {filename}\")\n",
    "\n",
    "    original_time = parse_date(filename).strftime(DATE_FORMAT)\n",
    "\n",
    "    if original_time is None:\n",
    "        raise ValueError(f\"Invalid filename for parsing timestamp: {filename}\")\n",
    "\n",
    "    # Load and return the file contents\n",
    "    with open(os.path.join(OUTPUT_DIRECTORY, filename), 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        return data, original_time\n",
    "\n",
    "def load_most_recent_anki_json():\n",
    "    \"\"\"Load the most recent anki json file from the outputs directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: The contents of the most recent anki json file\n",
    "    \"\"\"\n",
    "    # List all anki json files in the output directory\n",
    "    anki_files = [f for f in os.listdir(OUTPUT_DIRECTORY) if f.startswith(\"anki@\") and f.endswith(\".json\")]\n",
    "    \n",
    "    if not anki_files:\n",
    "        raise FileNotFoundError(\"No anki json files found in outputs directory\")\n",
    "    \n",
    "    most_recent = max(anki_files, key=parse_date)\n",
    "    print(f\"Loading most recent file: {most_recent}\")\n",
    "\n",
    "    return load_anki_json(most_recent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_words_to_update(debug: bool = False, from_to: Tuple[int, int] = (0, 10_000), verbose: bool = False):\n",
    "    # Fetch all Korean note IDs\n",
    "    note_ids = request_anki('findNotes', query=f'deck:\"{DECK_NAME}\"')\n",
    "\n",
    "    if debug:\n",
    "        notes_info = request_anki('notesInfo', notes=note_ids[:10])\n",
    "    else:\n",
    "        notes_info = request_anki('notesInfo', notes=note_ids[from_to[0]:from_to[1]])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"notes: {notes_info}\")\n",
    "\n",
    "    # Keep only those with empty IPA field\n",
    "    notes_info = [note for note in notes_info if get_ipa(note) == \"\"]\n",
    "    \n",
    "    # Return words\n",
    "    words_ids = {get_vocab(note): (note['noteId'], None) for note in notes_info} # The tuple contains the note ID and the IPA\n",
    "\n",
    "    return words_ids\n",
    "\n",
    "def extract_word_ipa__single(word, note_id, ipa):\n",
    "    \"\"\"Process a single word and return the results\"\"\"\n",
    "    try:\n",
    "        web_content = get_content(word, save_response=False)\n",
    "        if not web_content:\n",
    "            print(f\"Error fetching content for word {word}\")\n",
    "            return word, (note_id, ipa), None\n",
    "\n",
    "        result = extract_ipa_for_language(web_content, LANGUAGE, word)\n",
    "        if not result:\n",
    "            print(f\"Error extracting IPA for word {word}\")\n",
    "            return word, (note_id, ipa), None\n",
    "\n",
    "        return word, (note_id, result), result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing word {word}: {e}\")\n",
    "        return word, (note_id, ipa), None\n",
    "\n",
    "def update_card_ipa__single(word: str, note_id: int, ipa: str, extra_ipa: bool):\n",
    "    \"\"\"Update a single note's IPA fields\"\"\"\n",
    "    try:\n",
    "        # Leave the extra-IPA field empty in case\n",
    "        if extra_ipa == True:\n",
    "            updated_note = {\n",
    "                'id': note_id,\n",
    "                'fields': {\n",
    "                    'IPA': ipa,\n",
    "                    'Extra-IPA': \"True\",\n",
    "                }\n",
    "            }\n",
    "        elif extra_ipa == False:\n",
    "            updated_note = {\n",
    "                'id': note_id,\n",
    "                'fields': {\n",
    "                    'IPA': ipa,\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value for extra_ipa for {word}: should be either True or False, but is {extra_ipa}\")\n",
    "\n",
    "        # Update the note\n",
    "        request_anki('updateNoteFields', note=updated_note)\n",
    "        return word, True, None  # Success\n",
    "    except Exception as e:\n",
    "        return word, False, str(e)  # Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./outputs/-되다_response_text_content.html as HTML\n",
      "Korean section not found: -되다\n"
     ]
    }
   ],
   "source": [
    "# possible words: \"책\", \"저\", \"libro\", \"놀다\", \"오다\", \"돈\", \"돌\"\n",
    "\n",
    "if TEST_PARSING:\n",
    "    word = \"-되다\"\n",
    "    content = get_content(word, save_response=True)\n",
    "\n",
    "    # ipa = extract_ipa_for_language(content, \"italian\", word)\n",
    "    # if ipa:\n",
    "    #     print(ipa)\n",
    "\n",
    "    ipa = extract_ipa_for_language(content, \"korean\", word)\n",
    "    if ipa:\n",
    "        print(ipa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch And Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to update: 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:07<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./outputs/anki@20250212-205800.json as JSON\n"
     ]
    }
   ],
   "source": [
    "if TEST_CREATING:\n",
    "    # Fetch words\n",
    "    words_ids = fetch_words_to_update()\n",
    "    print(f\"Words to update: {len(words_ids)}\")\n",
    "\n",
    "    # Process words in parallel\n",
    "    results = Parallel(n_jobs=N_JOBS_EXTRACT)(\n",
    "        delayed(extract_word_ipa__single)(word, note_id, ipa) \n",
    "        for word, (note_id, ipa) in tqdm(words_ids.items())\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    skipped_dict = {}\n",
    "    updated_words = {}\n",
    "\n",
    "    for word, (note_id, result), success in results:\n",
    "        if not success:\n",
    "            skipped_dict[word] = (note_id, result)\n",
    "        else:\n",
    "            try:\n",
    "                ipa, extra_ipa = result\n",
    "                updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating word {word}: {e}\")\n",
    "                skipped_dict[word] = (note_id, result)\n",
    "\n",
    "    # Save the output\n",
    "    output = {\n",
    "        'skipped_words': skipped_dict,\n",
    "        'updated_words': updated_words\n",
    "    }\n",
    "\n",
    "    current_time = datetime.now().strftime(DATE_FORMAT)\n",
    "    save(output, f\"anki@{current_time}.json\")\n",
    "\n",
    "    len(updated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_UPDATING:\n",
    "    # Usage\n",
    "    anki_json, original_time = load_most_recent_anki_json()\n",
    "    updated_words = anki_json.get('updated_words', {})\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in updated_words.items()]\n",
    "\n",
    "    # Process in parallel with progress bar\n",
    "    results = Parallel(n_jobs=N_JOBS_UPDATE)(\n",
    "        delayed(update_card_ipa__single)(word, note_id, ipa, extra_ipa) \n",
    "        for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\")\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    success = []\n",
    "    errors = []\n",
    "    for word, status, error in results:\n",
    "        if status:\n",
    "            success.append(word)\n",
    "        else:\n",
    "            errors.append((word, error))\n",
    "\n",
    "    # Save the output\n",
    "    error_words = [word for word, error in errors]\n",
    "    after_skipped_words = {word: info for word, info in updated_words.items() if word in error_words}\n",
    "    after_updated_words = {word: info for word, info in updated_words.items() if word not in error_words}\n",
    "\n",
    "    after_output = {\n",
    "        \"skipped_words\": after_skipped_words,\n",
    "        \"updated_words\": after_updated_words,\n",
    "    }\n",
    "\n",
    "    save(after_output, f\"after_anki@{original_time}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_RETRY_UPDATING:\n",
    "    # Here we don't parallelize, since I only found errors originating from too many handles at the same time so far\n",
    "    after_anki_json, _ = load_anki_json(f\"after_anki@{original_time}.json\")\n",
    "\n",
    "    skipped_words = after_anki_json.get(\"skipped_words\", {})\n",
    "\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in skipped_words.items()]\n",
    "    \n",
    "    final_skipped_words = {}\n",
    "    final_updated_words = {}\n",
    "\n",
    "    for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\"):\n",
    "        try:\n",
    "            update_card_ipa__single(word, note_id, ipa, extra_ipa)\n",
    "            final_updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating word {word}: {e}\")\n",
    "            final_skipped_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            continue\n",
    "\n",
    "    # Save the output\n",
    "    final_output = {\n",
    "        \"skipped_words\": final_skipped_words,\n",
    "        \"updated_words\": final_updated_words,\n",
    "    }\n",
    "\n",
    "    save(final_output, f\"final_anki@{original_time}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still skipped words\n"
     ]
    }
   ],
   "source": [
    "after_anki_json, _ = load_anki_json(f\"after_anki@20250217-171308.json\")\n",
    "if len(after_anki_json[\"skipped_words\"]) == 0:\n",
    "    print(\"There are still skipped words\")\n",
    "else :\n",
    "    print(\"All words have been updated\")\n",
    "# skipped_words = after_anki_json.get(\"skipped_words\", {})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
