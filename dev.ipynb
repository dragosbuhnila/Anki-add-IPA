{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Tuple\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from utils.scraper import get_content, extract_ipa_for_language\n",
    "from utils.file import save\n",
    "from config import OUTPUT_DIRECTORY, ANKI_CONNECT_URL, DECK_NAME, LANGUAGE, VOCAB_FIELD, N_CORES, N_JOBS_EXTRACT, N_JOBS_UPDATE, DATE_FORMAT\n",
    "\n",
    "TEST_PARSING = True\n",
    "TEST_CREATING = True\n",
    "TEST_UPDATING = False\n",
    "TEST_RETRY_UPDATING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./outputs/-되다_response_text_content.html as HTML\n"
     ]
    }
   ],
   "source": [
    "# possible words: \"책\", \"저\", \"libro\", \"놀다\", \"오다\", \"돈\", \"돌\"\n",
    "\n",
    "if TEST_PARSING:\n",
    "    word = \"-되다\"\n",
    "    content = get_content(word, save_response=True)\n",
    "\n",
    "    # ipa = extract_ipa_for_language(content, \"italian\", word)\n",
    "    # if ipa:\n",
    "    #     print(ipa)\n",
    "\n",
    "    ipa = extract_ipa_for_language(content, \"korean\", word)\n",
    "    if ipa:\n",
    "        print(ipa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch And Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_words_to_update' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TEST_CREATING:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Fetch words\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     words_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_words_to_update\u001b[49m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords to update: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(words_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Process words in parallel\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fetch_words_to_update' is not defined"
     ]
    }
   ],
   "source": [
    "if TEST_CREATING:\n",
    "    # Fetch words\n",
    "    words_ids = fetch_words_to_update()\n",
    "    print(f\"Words to update: {len(words_ids)}\")\n",
    "\n",
    "    # Process words in parallel\n",
    "    results = Parallel(n_jobs=N_JOBS_EXTRACT)(\n",
    "        delayed(extract_word_ipa__single)(word, note_id, ipa) \n",
    "        for word, (note_id, ipa) in tqdm(words_ids.items())\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    skipped_dict = {}\n",
    "    updated_words = {}\n",
    "\n",
    "    for word, (note_id, result), success in results:\n",
    "        if not success:\n",
    "            skipped_dict[word] = (note_id, result)\n",
    "        else:\n",
    "            try:\n",
    "                ipa, extra_ipa = result\n",
    "                updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating word {word}: {e}\")\n",
    "                skipped_dict[word] = (note_id, result)\n",
    "\n",
    "    # Save the output\n",
    "    output = {\n",
    "        'skipped_words': skipped_dict,\n",
    "        'updated_words': updated_words\n",
    "    }\n",
    "\n",
    "    current_time = datetime.now().strftime(DATE_FORMAT)\n",
    "    save(output, f\"anki@{current_time}.json\")\n",
    "\n",
    "    len(updated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_UPDATING:\n",
    "    # Usage\n",
    "    anki_json, original_time = load_most_recent_anki_json()\n",
    "    updated_words = anki_json.get('updated_words', {})\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in updated_words.items()]\n",
    "\n",
    "    # Process in parallel with progress bar\n",
    "    results = Parallel(n_jobs=N_JOBS_UPDATE)(\n",
    "        delayed(update_card_ipa__single)(word, note_id, ipa, extra_ipa) \n",
    "        for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\")\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    success = []\n",
    "    errors = []\n",
    "    for word, status, error in results:\n",
    "        if status:\n",
    "            success.append(word)\n",
    "        else:\n",
    "            errors.append((word, error))\n",
    "\n",
    "    # Save the output\n",
    "    error_words = [word for word, error in errors]\n",
    "    after_skipped_words = {word: info for word, info in updated_words.items() if word in error_words}\n",
    "    after_updated_words = {word: info for word, info in updated_words.items() if word not in error_words}\n",
    "\n",
    "    after_output = {\n",
    "        \"skipped_words\": after_skipped_words,\n",
    "        \"updated_words\": after_updated_words,\n",
    "    }\n",
    "\n",
    "    save(after_output, f\"after_anki@{original_time}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_RETRY_UPDATING:\n",
    "    # Here we don't parallelize, since I only found errors originating from too many handles at the same time so far\n",
    "    after_anki_json, _ = load_anki_json(f\"after_anki@{original_time}.json\")\n",
    "\n",
    "    skipped_words = after_anki_json.get(\"skipped_words\", {})\n",
    "\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in skipped_words.items()]\n",
    "    \n",
    "    final_skipped_words = {}\n",
    "    final_updated_words = {}\n",
    "\n",
    "    for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\"):\n",
    "        try:\n",
    "            update_card_ipa__single(word, note_id, ipa, extra_ipa)\n",
    "            final_updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating word {word}: {e}\")\n",
    "            final_skipped_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            continue\n",
    "\n",
    "    # Save the output\n",
    "    final_output = {\n",
    "        \"skipped_words\": final_skipped_words,\n",
    "        \"updated_words\": final_updated_words,\n",
    "    }\n",
    "\n",
    "    save(final_output, f\"final_anki@{original_time}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still skipped words\n"
     ]
    }
   ],
   "source": [
    "after_anki_json, _ = load_anki_json(f\"after_anki@20250217-171308.json\")\n",
    "if len(after_anki_json[\"skipped_words\"]) == 0:\n",
    "    print(\"There are still skipped words\")\n",
    "else :\n",
    "    print(\"All words have been updated\")\n",
    "# skipped_words = after_anki_json.get(\"skipped_words\", {})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
