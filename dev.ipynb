{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import DATE_FORMAT, DECK_NAME, N_JOBS_EXTRACT, N_JOBS_UPDATE\n",
    "from utils.app import extract_word_ipa__single, fetch_words_to_update, update_card_ipa__single\n",
    "from utils.file import save\n",
    "from utils.scraper import extract_ipa_for_language, get_content\n",
    "from utils.utils import load_anki_json, load_most_recent_anki_json\n",
    "\n",
    "TEST_PARSING = True\n",
    "TEST_CREATING = False\n",
    "TEST_UPDATING = False\n",
    "TEST_RETRY_UPDATING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: ('/ˈno/', True)\n",
      "tengo: ('/ˈtenɡo/', False)\n",
      "comida: ('/koˈmida/', False)\n"
     ]
    }
   ],
   "source": [
    "# possible words: \"책\", \"저\", \"libro\", \"놀다\", \"오다\", \"돈\", \"돌\"\n",
    "import unicodedata\n",
    "from utils.utils import preprocess_word, is_word\n",
    "\n",
    "\n",
    "\n",
    "if TEST_PARSING:\n",
    "    phrase = \"No -tEngo comida? (2)\"\n",
    "    language = \"spanish\"\n",
    "\n",
    "    ipas = dict()\n",
    "    words = phrase.split()\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_word(word):\n",
    "            content = get_content(word)\n",
    "            ipa = extract_ipa_for_language(content, language, word, verbose=True)\n",
    "            if ipa:\n",
    "                ipas[word] = ipa\n",
    "            else:\n",
    "                ipas[word] = None\n",
    "\n",
    "    for word, ipa in ipas.items():\n",
    "        print(f\"{word}: {ipa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch And Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_CREATING:\n",
    "    # Fetch words\n",
    "    words_ids = fetch_words_to_update()\n",
    "    print(f\"Words to update: {len(words_ids)}\")\n",
    "\n",
    "    # Process words in parallel\n",
    "    results = Parallel(n_jobs=N_JOBS_EXTRACT)(\n",
    "        delayed(extract_word_ipa__single)(word, note_id, ipa) \n",
    "        for word, (note_id, ipa) in tqdm(words_ids.items())\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    skipped_dict = {}\n",
    "    updated_words = {}\n",
    "\n",
    "    for word, (note_id, result), success in results:\n",
    "        if not success:\n",
    "            skipped_dict[word] = (note_id, result)\n",
    "        else:\n",
    "            try:\n",
    "                ipa, extra_ipa = result\n",
    "                updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating word {word}: {e}\")\n",
    "                skipped_dict[word] = (note_id, result)\n",
    "\n",
    "    # Save the output\n",
    "    output = {\n",
    "        'skipped_words': skipped_dict,\n",
    "        'updated_words': updated_words\n",
    "    }\n",
    "\n",
    "    current_time = datetime.now().strftime(DATE_FORMAT)\n",
    "    save(output, f\"anki@{current_time}.json\")\n",
    "\n",
    "    len(updated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update and Retry Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_UPDATING:\n",
    "    # Usage\n",
    "    anki_json, original_time = load_most_recent_anki_json()\n",
    "    updated_words = anki_json.get('updated_words', {})\n",
    "\n",
    "    # Prepare the arguments for parallel processing\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in updated_words.items()]\n",
    "\n",
    "    # Process in parallel with progress bar\n",
    "    results = Parallel(n_jobs=N_JOBS_UPDATE)(\n",
    "        delayed(update_card_ipa__single)(word, note_id, ipa, extra_ipa) \n",
    "        for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\")\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    success = []\n",
    "    errors = []\n",
    "    for word, status, error in results:\n",
    "        if status:\n",
    "            success.append(word)\n",
    "        else:\n",
    "            errors.append((word, error))\n",
    "\n",
    "    # Save the output\n",
    "    error_words = [word for word, error in errors]\n",
    "    after_skipped_words = {word: info for word, info in updated_words.items() if word in error_words}\n",
    "    after_updated_words = {word: info for word, info in updated_words.items() if word not in error_words}\n",
    "\n",
    "    after_output = {\n",
    "        \"skipped_words\": after_skipped_words,\n",
    "        \"updated_words\": after_updated_words,\n",
    "    }\n",
    "\n",
    "    save(after_output, f\"after_anki@{original_time}.json\")\n",
    "\n",
    "if TEST_RETRY_UPDATING:\n",
    "    # Here we don't parallelize, since I only found errors originating from too many handles at the same time so far\n",
    "    after_anki_json, _ = load_anki_json(f\"after_anki@{original_time}.json\")\n",
    "\n",
    "    skipped_words = after_anki_json.get(\"skipped_words\", {})\n",
    "\n",
    "    args = [(word, info['note_id'], info['ipa'], info['extra_ipa']) \n",
    "            for word, info in skipped_words.items()]\n",
    "    \n",
    "    final_skipped_words = {}\n",
    "    final_updated_words = {}\n",
    "\n",
    "    for word, note_id, ipa, extra_ipa in tqdm(args, desc=\"Updating IPAs\"):\n",
    "        try:\n",
    "            update_card_ipa__single(word, note_id, ipa, extra_ipa)\n",
    "            final_updated_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating word {word}: {e}\")\n",
    "            final_skipped_words[word] = {\"note_id\": note_id, \"ipa\": ipa, \"extra_ipa\": extra_ipa}\n",
    "            continue\n",
    "\n",
    "    # Save the output\n",
    "    final_output = {\n",
    "        \"skipped_words\": final_skipped_words,\n",
    "        \"updated_words\": final_updated_words,\n",
    "    }\n",
    "\n",
    "    save(final_output, f\"final_anki@{original_time}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
